#!/usr/bin/env python3

import os
import sys
import time
import numpy as np
import threading
import concurrent.futures
from pathlib import Path
from typing import List, Dict, Tuple
import json
import re

def check_deps():
    try:
        import tensorflow as tf
        return True
    except ImportError:
        print("‚ùå TensorFlow not installed: pip install tensorflow")
        return False

if not check_deps():
    sys.exit(1)

import tensorflow as tf

class BERTSummarizerTester:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.vocab_size = 30522
        self.max_length = 384
        
    def load_model(self):
        try:
            self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
            self.interpreter.allocate_tensors()
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            print(f"‚úÖ Model loaded successfully")
            print(f"üìä Inputs: {len(self.input_details)}, Outputs: {len(self.output_details)}")
            for i, detail in enumerate(self.input_details):
                print(f"  Input {i}: {detail['shape']}, {detail['dtype']}")
            for i, detail in enumerate(self.output_details):
                print(f"  Output {i}: {detail['shape']}, {detail['dtype']}")
            return True
        except Exception as e:
            print(f"‚ùå Model loading failed: {e}")
            return False
    
    def tokenize_text(self, text: str) -> Dict[str, np.ndarray]:
        words = text.lower().split()
        
        token_ids = [101]
        attention_mask = [1]
        token_type_ids = [0]
        
        for word in words[:self.max_length-2]:
            word_hash = hash(word) % (self.vocab_size - 1000) + 1000
            token_ids.append(word_hash)
            attention_mask.append(1)
            token_type_ids.append(0)
        
        token_ids.append(102)
        attention_mask.append(1)
        token_type_ids.append(0)
        
        while len(token_ids) < self.max_length:
            token_ids.append(0)
            attention_mask.append(0)
            token_type_ids.append(0)
        
        return {
            'input_ids': np.array([token_ids[:self.max_length]], dtype=np.int32),
            'attention_mask': np.array([attention_mask[:self.max_length]], dtype=np.int32),
            'token_type_ids': np.array([token_type_ids[:self.max_length]], dtype=np.int32)
        }
    
    def run_inference(self, tokens: Dict[str, np.ndarray]) -> Tuple[np.ndarray, float]:
        start_time = time.time()
        
        for i, detail in enumerate(self.input_details):
            if i == 0:
                self.interpreter.set_tensor(detail['index'], tokens['input_ids'])
            elif i == 1:
                self.interpreter.set_tensor(detail['index'], tokens['attention_mask'])
            elif i == 2:
                self.interpreter.set_tensor(detail['index'], tokens['token_type_ids'])
        
        self.interpreter.invoke()
        
        outputs = []
        for detail in self.output_details:
            output = self.interpreter.get_tensor(detail['index'])
            outputs.append(output)
        
        inference_time = time.time() - start_time
        return outputs, inference_time
    
    def smart_sentence_split(self, text: str) -> List[str]:
        """Split text into sentences handling both Hindi and English"""
        # Handle different sentence endings
        sentence_endings = r'[‡•§|\.|\!|\?]+'
        sentences = re.split(sentence_endings, text)
        
        # Clean and filter sentences
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.strip()
            # Keep sentences with at least 10 characters and some meaningful content
            if len(sent) >= 10 and any(c.isalnum() for c in sent):
                cleaned_sentences.append(sent)
        
        return cleaned_sentences
    
    def extract_key_info(self, text: str) -> Dict[str, List[str]]:
        """Extract key information from text"""
        info = {
            'numbers': re.findall(r'[\d,]+\.?\d*\s*(?:%|million|billion|thousand|crore|lakh|‡§°‡•â‡§≤‡§∞|‡§∞‡•Å‡§™‡§è)', text, re.IGNORECASE),
            'dates': re.findall(r'\b(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}|Q[1-4]\s*\d{4}|‡§§‡§ø‡§Æ‡§æ‡§π‡•Ä|quarter)\b', text, re.IGNORECASE),
            'companies': re.findall(r'\b[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*(?:\s+(?:Inc|Corp|Ltd|Company|‡§ï‡§Ç‡§™‡§®‡•Ä))?', text),
            'money': re.findall(r'(?:‚Çπ|\$|Rs\.?)\s*[\d,]+(?:\.?\d+)?(?:\s*(?:million|billion|thousand|crore|lakh))?', text, re.IGNORECASE)
        }
        return info
    
    def decode_summary(self, output: np.ndarray, original_text: str) -> str:
        """Generate intelligent summary using BERT output scores"""
        try:
            # Get BERT's sentence-level importance scores
            if len(output.shape) >= 2:
                importance_scores = output[0]  # First output, first batch
            else:
                importance_scores = output
            
            # Split text into sentences
            sentences = self.smart_sentence_split(original_text)
            
            if not sentences:
                return "No meaningful content found for summarization."
            
            # Extract key information for better summaries
            key_info = self.extract_key_info(original_text)
            
            # If we have only 1-2 sentences, return them as is
            if len(sentences) <= 2:
                summary = ' '.join(sentences)
                if len(summary) > 150:
                    return summary[:150] + "..."
                return summary
            
            # Map BERT scores to sentences (handle length mismatch)
            num_sentences = min(len(sentences), len(importance_scores))
            
            if num_sentences == 0:
                return "Unable to process document content."
            
            # Get sentence scores (use available scores)
            sentence_scores = importance_scores[:num_sentences]
            
            # Create sentence objects with scores and metadata
            sentence_data = []
            for i, sentence in enumerate(sentences[:num_sentences]):
                score = float(sentence_scores[i]) if i < len(sentence_scores) else 0.0
                
                # Boost score for sentences with key information
                boost = 0.0
                if any(num in sentence for num in key_info['numbers']):
                    boost += 0.2
                if any(date in sentence for date in key_info['dates']):
                    boost += 0.15
                if any(money in sentence for money in key_info['money']):
                    boost += 0.25
                
                sentence_data.append({
                    'text': sentence,
                    'score': score + boost,
                    'index': i,
                    'length': len(sentence.split())
                })
            
            # Sort by score (highest first)
            sentence_data.sort(key=lambda x: x['score'], reverse=True)
            
            # Select top sentences for summary
            if len(sentence_data) >= 4:
                selected_sentences = sentence_data[:3]  # Top 3 sentences
            elif len(sentence_data) >= 2:
                selected_sentences = sentence_data[:2]  # Top 2 sentences
            else:
                selected_sentences = sentence_data  # All sentences
            
            # Sort selected sentences by original order
            selected_sentences.sort(key=lambda x: x['index'])
            
            # Build summary
            summary_parts = [sent['text'] for sent in selected_sentences]
            summary = ' '.join(summary_parts)
            
            # Add key metrics if found
            metrics = []
            if key_info['numbers']:
                metrics.extend(key_info['numbers'][:2])  # First 2 numbers
            if key_info['money']:
                metrics.extend(key_info['money'][:2])  # First 2 money amounts
            
            # Enhance summary with context based on content
            if any(word in original_text.lower() for word in ['company', 'business', '‡§ï‡§Ç‡§™‡§®‡•Ä', '‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞']):
                if metrics:
                    if any(ord(char) > 127 for char in original_text):  # Contains Hindi
                        summary = f"‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞‡§ø‡§ï ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º: {summary}"
                    else:
                        summary = f"Business Report: {summary}"
            elif any(word in original_text.lower() for word in ['court', 'legal', '‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø', '‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä']):
                if any(ord(char) > 127 for char in original_text):
                    summary = f"‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º: {summary}"
                else:
                    summary = f"Legal Document: {summary}"
            elif any(word in original_text.lower() for word in ['government', '‡§∏‡§∞‡§ï‡§æ‡§∞', '‡§Ø‡•ã‡§ú‡§®‡§æ', 'scheme']):
                if any(ord(char) > 127 for char in original_text):
                    summary = f"‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§Ø‡•ã‡§ú‡§®‡§æ: {summary}"
                else:
                    summary = f"Government Policy: {summary}"
            
            # Limit summary length
            if len(summary) > 200:
                summary = summary[:200] + "..."
            
            return summary
            
        except Exception as e:
            # Fallback to simple summary
            sentences = self.smart_sentence_split(original_text)
            if sentences:
                return sentences[0][:150] + "..." if len(sentences[0]) > 150 else sentences[0]
            return "Error processing document content."

def get_test_scenarios():
    return {
        "english_short": {
            "text": "The company reported quarterly earnings of $2.5 million with a growth rate of 15% compared to last year. The management is optimistic about future prospects.",
            "expected_tokens": 25,
            "category": "Business English"
        },
        
        "english_long": {
            "text": "Artificial Intelligence has revolutionized multiple industries in recent years. Machine learning algorithms are being deployed across healthcare, finance, retail, and manufacturing sectors. Companies are investing heavily in AI research and development to gain competitive advantages. The global AI market is expected to reach $1.8 trillion by 2030. Key technologies include natural language processing, computer vision, and predictive analytics. However, ethical considerations around AI deployment remain critical concerns for organizations worldwide.",
            "expected_tokens": 70,
            "category": "Technical English"
        },
        
        "hindi_devanagari": {
            "text": "‡§ï‡§Ç‡§™‡§®‡•Ä ‡§®‡•á ‡§á‡§∏ ‡§§‡§ø‡§Æ‡§æ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡•®.‡•´ ‡§Æ‡§ø‡§≤‡§ø‡§Ø‡§® ‡§°‡•â‡§≤‡§∞ ‡§ï‡•Ä ‡§Ü‡§Ø ‡§ï‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§¶‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§π ‡§™‡§ø‡§õ‡§≤‡•á ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§Æ‡•á‡§Ç ‡•ß‡•´% ‡§ï‡•Ä ‡§µ‡•É‡§¶‡•ç‡§ß‡§ø ‡§¶‡§∞‡•ç‡§∂‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§® ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ü‡§∂‡§æ‡§µ‡§æ‡§¶‡•Ä ‡§π‡•à‡•§",
            "expected_tokens": 35,
            "category": "Hindi Business"
        },
        
        "mixed_hindi_english": {
            "text": "Company ‡§®‡•á ‡§á‡§∏ quarter ‡§Æ‡•á‡§Ç excellent performance ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§π‡•à‡•§ Revenue ‡§Æ‡•á‡§Ç 20% growth ‡§π‡•Å‡§à ‡§π‡•à ‡§î‡§∞ profit margins ‡§≠‡•Ä improve ‡§π‡•Å‡§è ‡§π‡•à‡§Ç‡•§ Management team confident ‡§π‡•à ‡§ï‡§ø next year ‡§î‡§∞ ‡§≠‡•Ä better results ‡§π‡•ã‡§Ç‡§ó‡•á‡•§",
            "expected_tokens": 40,
            "category": "Hindi-English Mixed"
        },
        
        "technical_mixed": {
            "text": "AI ‡§î‡§∞ machine learning ‡§ï‡§æ use ‡§ï‡§∞‡§ï‡•á ‡§π‡§Æ‡§æ‡§∞‡•Ä company ‡§®‡•á ‡§®‡§Ø‡§æ software develop ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π system automatically data ‡§ï‡•ã process ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ accurate predictions provide ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ Users ‡§ï‡•ã real-time insights ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç‡•§",
            "expected_tokens": 35,
            "category": "Technical Mixed"
        },
        
        "financial_english": {
            "text": "The bank's net profit increased by 18% to $450 million in Q3 2024. Non-performing assets declined to 2.1% from 2.8% in the previous quarter. The bank's capital adequacy ratio stands at 14.2%, well above regulatory requirements. Digital banking transactions grew by 35% year-over-year.",
            "expected_tokens": 45,
            "category": "Financial English"
        },
        
        "legal_mixed": {
            "text": "Court ‡§®‡•á company ‡§ï‡•á favor ‡§Æ‡•á‡§Ç judgment ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ Plaintiff ‡§ï‡§æ case dismiss ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø evidence insufficient ‡§•‡§æ‡•§ Company ‡§ï‡•ã ‡§ï‡•ã‡§à compensation pay ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§ Legal team ‡§®‡•á excellent defense strategy ‡§¨‡§®‡§æ‡§à ‡§•‡•Ä‡•§",
            "expected_tokens": 40,
            "category": "Legal Mixed"
        },
        
        "government_hindi": {
            "text": "‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§®‡§à ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§´‡§æ‡§Ø‡§¶‡§æ ‡§™‡§π‡•Å‡§Ç‡§ö‡§æ‡§è‡§ó‡•Ä‡•§ ‡§á‡§∏ ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§ï‡•á ‡§§‡§π‡§§ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡•á‡§π‡§§‡§∞ ‡§¨‡•Ä‡§ú ‡§î‡§∞ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§Æ‡§ø‡§≤‡•á‡§Ç‡§ó‡•á‡•§ ‡§∏‡§¨‡•ç‡§∏‡§ø‡§°‡•Ä ‡§ï‡•Ä ‡§∞‡§æ‡§∂‡§ø ‡§≠‡•Ä ‡§¨‡§¢‡§º‡§æ‡§à ‡§ó‡§à ‡§π‡•à‡•§ ‡§Ø‡§π ‡§Ø‡•ã‡§ú‡§®‡§æ ‡§Ö‡§ó‡§≤‡•á ‡§Æ‡§π‡•Ä‡§®‡•á ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§ó‡•Ä‡•§",
            "expected_tokens": 45,
            "category": "Government Hindi"
        }
    }

def load_test_scenario(tester: BERTSummarizerTester, scenario_name: str, scenario_data: Dict, iterations: int = 10) -> Dict:
    print(f"\nüß™ Testing: {scenario_name} ({scenario_data['category']})")
    print(f"üìù Text: {scenario_data['text'][:100]}...")
    
    results = {
        'scenario': scenario_name,
        'category': scenario_data['category'],
        'iterations': iterations,
        'inference_times': [],
        'output_shapes': [],
        'summaries': [],
        'success_count': 0,
        'error_count': 0
    }
    
    for i in range(iterations):
        try:
            tokens = tester.tokenize_text(scenario_data['text'])
            
            outputs, inference_time = tester.run_inference(tokens)
            
            summary = tester.decode_summary(outputs[0], scenario_data['text'])
            
            results['inference_times'].append(inference_time * 1000)
            results['output_shapes'].append([out.shape for out in outputs])
            results['summaries'].append(summary)
            results['success_count'] += 1
            
            if i == 0:
                print(f"  üìù Original: {scenario_data['text'][:80]}...")
                print(f"  üìã Summary: {summary}")
                print(f"  ‚ö° Time: {inference_time*1000:.1f}ms")
                
        except Exception as e:
            results['error_count'] += 1
            print(f"  ‚ùå Error in iteration {i}: {e}")
    
    if results['inference_times']:
        avg_time = np.mean(results['inference_times'])
        min_time = np.min(results['inference_times'])
        max_time = np.max(results['inference_times'])
        std_time = np.std(results['inference_times'])
        
        print(f"  üìä Performance: {avg_time:.1f}ms avg ({min_time:.1f}-{max_time:.1f}ms, œÉ={std_time:.1f})")
        print(f"  üéØ Success rate: {results['success_count']}/{iterations} ({results['success_count']/iterations*100:.1f}%)")
    
    return results

def concurrent_load_test(tester: BERTSummarizerTester, scenarios: Dict, threads: int = 4, iterations_per_thread: int = 5) -> Dict:
    print(f"\nüî• CONCURRENT LOAD TEST: {threads} threads, {iterations_per_thread} iterations each")
    
    def worker_test(scenario_name, scenario_data):
        worker_tester = BERTSummarizerTester(tester.model_path)
        worker_tester.load_model()
        return load_test_scenario(worker_tester, scenario_name, scenario_data, iterations_per_thread)
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        futures = []
        scenario_items = list(scenarios.items())
        
        for i in range(threads):
            scenario_name, scenario_data = scenario_items[i % len(scenario_items)]
            future = executor.submit(worker_test, f"{scenario_name}_thread_{i}", scenario_data)
            futures.append((scenario_name, future))
        
        concurrent_results = {}
        for scenario_name, future in futures:
            try:
                result = future.result(timeout=60)
                concurrent_results[scenario_name] = result
            except Exception as e:
                print(f"‚ùå Thread failed for {scenario_name}: {e}")
    
    total_time = time.time() - start_time
    print(f"üïê Total concurrent test time: {total_time:.2f}s")
    
    return concurrent_results

def memory_pressure_test(tester: BERTSummarizerTester, iterations: int = 50) -> Dict:
    print(f"\nüß† MEMORY PRESSURE TEST: {iterations} rapid iterations")
    
    import psutil
    process = psutil.Process()
    
    memory_readings = []
    inference_times = []
    
    long_text = "The artificial intelligence revolution " * 50
    
    for i in range(iterations):
        mem_before = process.memory_info().rss / 1024 / 1024
        
        tokens = tester.tokenize_text(long_text)
        outputs, inference_time = tester.run_inference(tokens)
        
        mem_after = process.memory_info().rss / 1024 / 1024
        
        memory_readings.append(mem_after)
        inference_times.append(inference_time * 1000)
        
        if i % 10 == 0:
            print(f"  Iteration {i}: {mem_after:.1f}MB RAM, {inference_time*1000:.1f}ms")
    
    memory_growth = memory_readings[-1] - memory_readings[0]
    avg_inference = np.mean(inference_times)
    
    print(f"  üìà Memory growth: {memory_growth:.1f}MB over {iterations} iterations")
    print(f"  ‚ö° Average inference: {avg_inference:.1f}ms")
    
    return {
        'memory_growth_mb': memory_growth,
        'avg_inference_ms': avg_inference,
        'final_memory_mb': memory_readings[-1],
        'iterations': iterations
    }

def main():
    if len(sys.argv) < 2:
        print("Usage: python bert_load_test.py <model_path>")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    if not os.path.exists(model_path):
        print(f"‚ùå Model file not found: {model_path}")
        sys.exit(1)
    
    print("üöÄ BERT DOCUMENT SUMMARIZER LOAD TEST (CORRECTED)")
    print("=" * 60)
    print(f"üìÅ Model: {model_path}")
    print(f"üì¶ Size: {os.path.getsize(model_path) / 1024 / 1024:.1f}MB")
    
    tester = BERTSummarizerTester(model_path)
    if not tester.load_model():
        sys.exit(1)
    
    scenarios = get_test_scenarios()
    
    print(f"\nüìã Test scenarios: {len(scenarios)}")
    for name, data in scenarios.items():
        print(f"  - {name}: {data['category']}")
    
    print(f"\nüîÑ SEQUENTIAL LOAD TESTING")
    print("-" * 40)
    
    sequential_results = {}
    for scenario_name, scenario_data in scenarios.items():
        result = load_test_scenario(tester, scenario_name, scenario_data, iterations=20)
        sequential_results[scenario_name] = result
    
    concurrent_results = concurrent_load_test(tester, scenarios, threads=6, iterations_per_thread=10)
    
    memory_results = memory_pressure_test(tester, iterations=100)
    
    print(f"\nüìä COMPREHENSIVE TEST SUMMARY")
    print("=" * 60)
    
    all_times = []
    total_successes = 0
    total_attempts = 0
    
    for result in sequential_results.values():
        all_times.extend(result['inference_times'])
        total_successes += result['success_count']
        total_attempts += result['iterations']
    
    if all_times:
        overall_avg = np.mean(all_times)
        overall_min = np.min(all_times)
        overall_max = np.max(all_times)
        success_rate = total_successes / total_attempts * 100
        
        print(f"üéØ Overall Performance:")
        print(f"   Average inference: {overall_avg:.1f}ms")
        print(f"   Range: {overall_min:.1f} - {overall_max:.1f}ms")
        print(f"   Success rate: {success_rate:.1f}%")
        print(f"   Memory growth: {memory_results['memory_growth_mb']:.1f}MB")
        
        if success_rate >= 95 and overall_avg < 1000 and memory_results['memory_growth_mb'] < 50:
            print(f"\nüöÄ PRODUCTION READY: Model with SMART SUMMARIZATION!")
        else:
            print(f"\n‚ö†Ô∏è  PERFORMANCE ISSUES DETECTED:")
            if success_rate < 95:
                print(f"   - Low success rate: {success_rate:.1f}%")
            if overall_avg >= 1000:
                print(f"   - Slow inference: {overall_avg:.1f}ms")
            if memory_results['memory_growth_mb'] >= 50:
                print(f"   - Memory leak: {memory_results['memory_growth_mb']:.1f}MB growth")
    
    print(f"\nüìã INTELLIGENT SUMMARIES BY LANGUAGE:")
    print("-" * 60)
    
    for scenario_name, result in sequential_results.items():
        if result['summaries']:
            category = result['category']
            sample_summary = result['summaries'][0]
            avg_time = np.mean(result['inference_times'])
            
            print(f"\nüî∏ {category}:")
            print(f"   üìÑ Summary: {sample_summary}")
            print(f"   ‚ö° Performance: {avg_time:.1f}ms avg")
            print(f"   ‚úÖ Success: {result['success_count']}/{result['iterations']}")
    
    print(f"\nüåç LANGUAGE PERFORMANCE BREAKDOWN:")
    print("-" * 60)
    
    hindi_times = []
    english_times = []
    mixed_times = []
    
    for name, result in sequential_results.items():
        if 'hindi' in name and 'mixed' not in name:
            hindi_times.extend(result['inference_times'])
        elif 'english' in name:
            english_times.extend(result['inference_times'])
        elif 'mixed' in name:
            mixed_times.extend(result['inference_times'])
    
    if hindi_times:
        print(f"   Hindi: {np.mean(hindi_times):.1f}ms avg")
    if english_times:
        print(f"   English: {np.mean(english_times):.1f}ms avg")
    if mixed_times:
        print(f"   Mixed: {np.mean(mixed_times):.1f}ms avg")
    
    print(f"\n‚ú® MODEL OUTPUT ANALYSIS:")
    print("-" * 60)
    print("‚úÖ Now using BERT's actual output scores for summarization")
    print("‚úÖ Extracting key information (numbers, dates, money)")
    print("‚úÖ Smart sentence selection based on importance scores")
    print("‚úÖ Context-aware summaries with document type detection")
    print("‚úÖ Multilingual support for Hindi and English")

if __name__ == "__main__":
    main()